# ðŸ§  SentiAid â€“ Speech to Animated Sign Language Interpreter

**SentiAid** is an AI-powered solution designed to bridge the communication gap between the Deaf and Hard-of-Hearing (DHH) community and the hearing world. Our system converts **spoken or textual input** into **real-time animated Indian Sign Language (ISL)** using deep learning, keypoint detection, and pose-based animation.

> ðŸŒ **Built With**: TensorFlow, Mediapipe, VideoMAE, LangGraph, LangChain, Transformers, IndicTrans2, LLM Agentic Frameworks

---

## ðŸš€ Project Objective

Develop a robust, scalable, and inclusive AI pipeline that:

* Converts **speech or text to ISL**
* Animates signs using pose/keypoint-based avatars
* Supports **22 Indian languages** (Scheduled)
* Ensures **accessibility** in education, workplaces, and public spaces

---

## âœ¨ Key Features

* ðŸŽ™ï¸ **Speech-to-Text**: Using Whisper or IndicTTS
* ðŸ“ **Text-to-Gloss**: With LangGraph + Agentic LLMs
* ðŸ” **Gloss-to-ISL Poses**: Generated using masked motion templates
* ðŸ§â€â™‚ï¸ **Avatar Animation**: Via Mediapipe keypoints and Blender/Three.js
* ðŸŒ **Multilingual Support**: Enabled by IndicTrans2
* ðŸ“± **Cross-Platform UI**: Accessibility-first mobile-friendly web app

---

## ðŸ§  System Architecture

```mermaid
graph TD
  A[Speech Input] --> B[Whisper / IndicTTS]
  B --> C[Text Output]
  C --> D[LangGraph LLM Agent â†’ Gloss]
  D --> E[Gloss â†’ Pose Sequence Model LSTM/Transformer]
  E --> F[Masked Pose Templates + ISL Mapping]
  F --> G[Animated Avatar Three.js / Blender / Mediapipe]
```

---

## ðŸ“Š Dataset: INCLUDE

**[INCLUDE Dataset â€“ Zenodo](https://zenodo.org/records/4010759)**
Provided by AI4Bharat, this dataset contains 4,292 videos of ISL signs:

* **Training Set**: 3,475 videos
* **Testing Set**: 817 videos
* Each video captures a single ISL sign, performed by deaf students from St. Louis School for the Deaf, Adyar, Chennai.

---

## ðŸ” Model Architectures

### 1. **LSTM-Based Model** (Dynamic Gesture Learning)

* **Input**: Mediapipe-extracted pose keypoints
* **Architecture**:

  * `TimeDistributed(Dense)` to extract spatial patterns
  * `LSTM` layers to capture temporal gesture transitions
* **Use Case**: Classifying ISL signs from real-time keypoints

### 2. **Transformer-Based Model** (Context-Aware)

* **Backbone**: Adapted from [VideoMAE](https://arxiv.org/abs/2203.12602)
* **Features**:

  * Warmup Scheduler for stable convergence
  * `AdamW` optimizer for generalization
  * `ReduceLROnPlateau` for dynamic learning rates
  * Finetuning on temporal gesture datasets

---

## ðŸŽ¯ Dual-Mode Solution Approach

### 1. **Sign âž¡ï¸ Text**

* **Pipeline**:

  * Video input â†’ Pose extraction (Mediapipe)
  * Pose â†’ Classifier (LSTM/Transformer)
  * Classification â†’ Text (via LangChain Agent)

### 2. **Text âž¡ï¸ Sign**

* **Pipeline**:

  * Natural Language Input â†’ Gloss (LLM Agent)
  * Gloss â†’ Pose Templates (masked sequences)
  * Output: 3D animated ISL avatar

---

## âœ… Current Progress

* âœ… LSTM model for ISL recognition
* âœ… Transformer encoder tested on INCLUDE dataset
* âœ… Agentic LangChain pipeline integrated
* âœ… Gloss-to-Pose pipeline functioning
* âœ… Accessibility-focused multilingual web app live: [https://www.sentiaid.co.in/](https://www.sentiaid.co.in/)

---

## ðŸ”­ Future Roadmap

### ðŸ”§ Model Optimization

* Benchmark LSTM and Transformer on dynamic sign sequences
* Develop a **hybrid model**: Local (LSTM) + Global (Transformer)
* Apply quantization and ONNX conversion for **real-time inference**

### ðŸ§¾ Dataset Expansion

* Add ISL phrases from **banking, health, education**
* Include **regional ISL dialects**
* Capture **facial cues, expressions, and gaze data**

### ðŸ“± Mobile App & Accessibility

* Merge Speech â†’ Text â†’ Sign flow in app
* Enable **offline functionality**
* Real-time 3D avatar rendering (WebGL/Blender pipeline)

### ðŸ› ï¸ API / SDK Development

* Public **REST API** for ISL translation
* **Plug-and-play SDKs** for EdTech & HR applications

### ðŸ“‹ Research & Publishing

* Submit papers to **ACL, COLING, EMNLP**
* Comparative study: LSTM vs Transformer for gesture decoding

---

## ðŸ”— Key Resources

* ðŸ“Š [INCLUDE Dataset (Zenodo)](https://zenodo.org/records/4010759)
* ðŸ§  [VideoMAE (HuggingFace)](https://huggingface.co/MCG-NJU/videomae-base)
* ðŸ‡®ðŸ‡³ [AI4Bharat NLP Models](https://huggingface.co/ai4bharat)
* ðŸ§¬ [LangChain Documentation](https://python.langchain.com/docs/)
* ðŸ¤– [LangGraph Agentic Workflows](https://python.langchain.com/docs/langgraph/)
* ðŸ§ [ISLRTC â€“ History of Indian Sign Language](https://islrtc.nic.in/history-0)

---

## ðŸ’¡ Example Code: LSTM Pose Classifier (Simplified)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import TimeDistributed, Dense, LSTM, Dropout

def create_lstm_model(input_shape, num_classes):
    model = Sequential()
    model.add(TimeDistributed(Dense(128), input_shape=input_shape))
    model.add(Dropout(0.3))
    model.add(LSTM(64, return_sequences=True))
    model.add(LSTM(32))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    return model

# Example usage:
# input_shape = (sequence_length, num_keypoints)
# num_classes = total number of ISL signs in your dataset
```

---

> **SentiAid** is committed to transforming accessibility through intelligent, inclusive technology â€” giving a voice to gestures and empowering the Deaf and Hard-of-Hearing community across India.

---

Would you like me to help design your **research paper**, **poster**, or a **pitch deck** using this summary?
